# AI Interview Simulator

A Generative AIâ€“powered application that simulates a **realistic interview cycle** by generating role-specific questions, evaluating candidate answers, and providing structured feedback.

This project focuses on **system design, LLM control, and explainability**, rather than feature bloat.

---

## ğŸ” Problem Statement

Candidates often lack access to structured interview practice that provides:
- Context-aware questions  
- Objective evaluation  
- Actionable feedback  

Most existing tools either:
- Only generate questions, or  
- Provide unstructured, generic feedback  

This project addresses that gap by modeling a **single interview cycle** with clear separation between **question generation, evaluation, and feedback**.

---

## âœ… Solution Overview

The AI Interview Simulator implements a **controller + agent-based architecture** where each component has a single responsibility:

- Generate an interview question based on role and difficulty
- Evaluate the candidateâ€™s answer using a structured rubric
- Convert evaluation into clear, user-friendly feedback

The system is intentionally scoped to **one complete interview cycle** to prioritize depth, correctness, and explainability.

---

## ğŸ§  Architecture

Controller (main / app)
â†“
Question Generator Agent
â†“
Evaluation Agent
â†“
Feedback Agent
â†“
LLM Client (Groq API)

---

## ğŸ“Š Evaluation Rubric

Candidate answers are evaluated using predefined criteria to ensure consistency and explainability:

- Technical Accuracy (0â€“5): Correctness of concepts and reasoning
- Completeness (0â€“5): Coverage of key aspects expected in an ideal answer
- Clarity & Structure (0â€“5): Logical flow and articulation
- Communication Quality (0â€“5): Confidence, conciseness, and relevance

Each score is accompanied by a short justification generated by the Evaluation Agent.

---

### Design Principles
- Separation of concerns  
- Reusable LLM abstraction  
- Prompt-controlled agent behavior  
- Explainable evaluation flow

All decision-making and evaluation logic resides in agents; the controller layer is intentionally kept thin.

---

## ğŸ§© Components & Responsibilities

### `main.py`
- CLI controller for the interview flow
- Handles user input and output
- Orchestrates agent execution

### `app.py`
- Streamlit-based web interface
- UI layer only (no AI logic)
- Calls underlying agents

### `core/llm_client.py`
- Centralized communication with the LLM
- Abstracts Groq API usage
- Keeps the system model-agnostic

### `agents/question_agent.py`
- Generates role- and difficulty-specific interview questions

### `agents/evaluation_agent.py`
- Evaluates candidate answers
- Produces structured output (score, strengths, gaps)

### `agents/feedback_agent.py`
- Converts raw evaluation into actionable feedback

---

## ğŸ”„ Runtime Flow

1. User selects role and difficulty  
2. Question Generator Agent creates an interview question  
3. User submits an answer  
4. Evaluation Agent scores and analyzes the answer  
5. Feedback Agent generates improvement guidance  
6. One complete interview cycle is completed  

---

## ğŸ§ª Example Interview Cycle (Simplified)

Role: Backend Developer  
Difficulty: Medium  

Question:
"Explain how indexing works in relational databases and its trade-offs."

Candidate Answer:
"..."

Evaluation Output:
- Technical Accuracy: 4/5
- Completeness: 3/5
- Clarity: 4/5

Feedback Summary:
Strong conceptual understanding, but lacks discussion on write-performance trade-offs.

---

## ğŸ› ï¸ Tech Stack

- Python  
- Streamlit  
- Groq API (LLM backend)  
- LLaMA-based models  
- Modular agent-based architecture  

---

ğŸ§  LLM Control & Prompting Strategy

To avoid generic or hallucinated feedback, the system uses:

- Prompt-constrained output formats (structured fields only)
- Explicit scoring rubrics embedded in evaluation prompts
- Role and difficulty conditioning for question generation
- Instructional separation between question, evaluation, and feedback prompts

Each agent operates with a narrowly defined responsibility to reduce ambiguity and improve output reliability.

---

## ğŸŒ Deployment

The application is deployed on **Streamlit Cloud**.

- Source code is hosted on GitHub  
- API keys are managed using environment secrets  
- No sensitive data is hardcoded  

---

## ğŸ¯ Scope & Design Decisions

### Why only one interview question?
The project intentionally focuses on a **single interview cycle** to:
- Ensure high-quality evaluation logic  
- Maintain explainability  
- Avoid shallow multi-feature implementation  

The architecture is **fully extensible** and can support:
- Multi-round interviews  
- Session memory  
- Adaptive difficulty  
- Persistent scoring  

---

## â“ Why Not Just Use ChatGPT?

While general-purpose chatbots can generate interview questions, this system enforces:

- Structured interview flow
- Predefined evaluation criteria
- Consistent scoring across candidates
- Separation of generation, evaluation, and feedback logic

This makes the simulator closer to a real interview assessment system rather than an open-ended conversation.

---

## âš ï¸ Limitations

- Single-question interview cycle  
- No long-term session memory  
- No database persistence  
- Evaluation quality depends on LLM reasoning  

These are deliberate design trade-offs made to prioritize evaluation quality and system clarity.

---

## ğŸš€ Future Enhancements

- Multi-round interview sessions  
- Adaptive difficulty based on performance  
- Session-level scoring and analytics  
- Interview-type customization (HR, system design, DSA)  
- Persistent user profiles  

---

## ğŸ§  Key Takeaway

This project demonstrates how to **design, control, and deploy a Generative AI system for evaluation tasks**, not just text generation.

It emphasizes:
- Architecture over UI  
- Depth over feature count  
- Explainability over black-box behavior  

---

## ğŸ‘¤ Author

**Aryan Nigam**  
B.Tech CSE | Generative AI & Backend Systems
